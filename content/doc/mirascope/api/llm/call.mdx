---
# AUTO-GENERATED API DOCUMENTATION - DO NOT EDIT
title: mirascope.llm.call
description: API documentation for mirascope.llm.call
---

# mirascope.llm.call

<ApiType type="Alias" />

<ApiSignature>
call(provider: Provider | LocalProvider, model: str, stream: bool, tools: list[BaseTool | Callable], response_model: BaseModel | BaseType, output_parser: Callable[[CallResponse | ResponseModelT], Any], json_mode: bool, client: object, call_params: CommonCallParams)
</ApiSignature>

## Description

A decorator for making provider-agnostic LLM API calls with a typed function.

usage docs: learn/calls.md

This decorator enables writing provider-agnostic code by wrapping a typed function 
that can call any supported LLM provider's API. It parses the prompt template of 
the wrapped function as messages and templates the input arguments into each message's 
template.

Example:

```python
from ..llm import call


@call(provider="openai", model="gpt-4o-mini")
def recommend_book(genre: str) -> str:
    return f"Recommend a {genre} book"


response = recommend_book("fantasy")
print(response.content)
```

Args:
    provider (Provider | LocalProvider): The LLM provider to use
        (e.g., "openai", "anthropic").
    model (str): The model to use for the specified provider (e.g., "gpt-4o-mini").
    stream (bool): Whether to stream the response from the API call.  
    tools (list[BaseTool | Callable]): The tools available for the LLM to use.
    response_model (BaseModel | BaseType): The response model into which the response
        should be structured.
    output_parser (Callable[[CallResponse | ResponseModelT], Any]): A function for
        parsing the call response whose value will be returned in place of the
        original call response.
    json_mode (bool): Whether to use JSON Mode.
    client (object): An optional custom client to use in place of the default client.
    call_params (CommonCallParams): Provider-specific parameters to use in the API call.

Returns:
    decorator (Callable): A decorator that transforms a typed function into a
        provider-agnostic LLM API call that returns standardized response types
        regardless of the underlying provider used.

<ParametersTable
  parameters={[
  {
    "name": "provider",
    "type": "Provider | LocalProvider",
    "description": "The LLM provider to use\n(e.g., \"openai\", \"anthropic\")."
  },
  {
    "name": "model",
    "type": "str",
    "description": "The model to use for the specified provider (e.g., \"gpt-4o-mini\")."
  },
  {
    "name": "stream",
    "type": "bool",
    "description": "Whether to stream the response from the API call.  "
  },
  {
    "name": "tools",
    "type": "list[BaseTool | Callable]",
    "description": "The tools available for the LLM to use."
  },
  {
    "name": "response_model",
    "type": "BaseModel | BaseType",
    "description": "The response model into which the response\nshould be structured."
  },
  {
    "name": "output_parser",
    "type": "Callable[[CallResponse | ResponseModelT], Any]",
    "description": "A function for\nparsing the call response whose value will be returned in place of the\noriginal call response."
  },
  {
    "name": "json_mode",
    "type": "bool",
    "description": "Whether to use JSON Mode."
  },
  {
    "name": "client",
    "type": "object",
    "description": "An optional custom client to use in place of the default client."
  },
  {
    "name": "call_params",
    "type": "CommonCallParams",
    "description": "Provider-specific parameters to use in the API call."
  }
]}
/>

<ReturnType
  type="Callable"
  description="A decorator that transforms a typed function into a\nprovider-agnostic LLM API call that returns standardized response types\nregardless of the underlying provider used."
/>


