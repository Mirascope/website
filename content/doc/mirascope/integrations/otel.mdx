---
title: OpenTelemetry Integration
description: Integration with OpenTelemetry for distributed tracing
---

# OpenTelemetry

Mirascope provides out-of-the-box integration with [OpenTelemetry](https://opentelemetry.io/).

You can install the necessary packages directly or using the `otel` extras flag:

```bash
pip install "mirascope[otel]"
```

You can then use the `with_otel` decorator to automatically create spans for LLM calls:

<TabbedSection>
<Tab value="Shorthand">
```python
from mirascope import llm
from mirascope.integrations.otel import configure, with_otel # [!code highlight]

configure() # [!code highlight]


@with_otel() # [!code highlight]
@llm.call(provider="$PROVIDER", model="$MODEL")
def recommend_book(genre: str) -> str:
    return f"Recommend a {genre} book."


print(recommend_book("fantasy"))

```
</Tab>
<Tab value="Template">
```python
from mirascope import llm, prompt_template
from mirascope.integrations.otel import configure, with_otel # [!code highlight]

configure() # [!code highlight]


@with_otel() # [!code highlight]
@llm.call(provider="$PROVIDER", model="$MODEL")
@prompt_template("Recommend a {genre} book")
def recommend_book(genre: str): ...


print(recommend_book("fantasy"))

```
</Tab>
</TabbedSection>

This will give you:

* A span around the `recommend_book` function that captures the prompt template and parameters
* Distributed tracing for LLM calls integrated with your existing OpenTelemetry setup
* Metrics for token usage and response time

<Note defaultOpen={false} collapsible={true} title="Handling streams">
  When logging streams, the span will not be logged until the stream has been exhausted. This is a function of how streaming works.
</Note>

