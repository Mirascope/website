---
title: Langfuse Integration
description: Integration with Langfuse for LLM observability
---

# Langfuse

Mirascope provides out-of-the-box integration with [Langfuse](https://langfuse.com/).

You can install the necessary packages directly or using the `langfuse` extras flag:

```bash
pip install "mirascope[langfuse]"
```

You can then use the `with_langfuse` decorator to automatically log calls:

<TabbedSection>
<Tab value="Shorthand">
```python
from mirascope import llm
from mirascope.integrations.langfuse import with_langfuse # [!code highlight]


@with_langfuse() # [!code highlight]
@llm.call(provider="$PROVIDER", model="$MODEL")
def recommend_book(genre: str) -> str:
    return f"Recommend a {genre} book."


print(recommend_book("fantasy"))

```
</Tab>
<Tab value="Template">
```python
from mirascope import llm, prompt_template
from mirascope.integrations.langfuse import with_langfuse # [!code highlight]


@with_langfuse() # [!code highlight]
@llm.call(provider="$PROVIDER", model="$MODEL")
@prompt_template("Recommend a {genre} book")
def recommend_book(genre: str): ...


print(recommend_book("fantasy"))

```
</Tab>
</TabbedSection>

This will give you:

* A trace around the `recommend_book` function that captures items like the prompt template, and input/output attributes and more.
* Human-readable display of the conversation with the agent
* Details of the response, including the number of tokens used

<Info defaultOpen={false} collapsible={true} title="Example trace">
  ![langfuse-trace](/assets/blog/langfuse-integration/mirascope_langfuse_trace.png)
</Info>

<Note defaultOpen={false} collapsible={true} title="Handling streams">
  When logging streams, the span will not be logged until the stream has been exhausted. This is a function of how streaming works.

  You will also need to set certain `call_params` for usage to be tracked for certain providers (such as OpenAI).
</Note>