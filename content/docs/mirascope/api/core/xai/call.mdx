---
# AUTO-GENERATED API DOCUMENTATION - DO NOT EDIT
title: mirascope.core.xai.call
description: API documentation for mirascope.core.xai.call
---

# mirascope.core.xai.call

<ApiType type="Alias" />

<ApiSignature>
call(model: str, stream: bool, tools: list[BaseTool | Callable], response_model: BaseModel | BaseType, output_parser: Callable[[OpenAICallResponse | ResponseModelT], Any], json_mode: bool, client: None, call_params: OpenAICallParams)
</ApiSignature>

## Description

A decorator for calling the xAI API with a typed function.

<Info title="Usage">

[Calls](/docs/mirascope/learn/calls)

</Info>


This decorator is used to wrap a typed function that calls the xAI API. It parses
the prompt template of the wrapped function as the messages array and templates the input
arguments for the function into each message's template.

Example:

```python
from mirascope.core import prompt_template
from mirascope.core.xai import xai_call


@xai_call("grok-2-latest")
def recommend_book(genre: str) -> str:
    return f"Recommend a {genre} book"

response = recommend_book("fantasy")
print(response.content)
```

Args:
    model (str): The model to use in the API call.
    stream (bool): Whether to stream the response from the API call.
    tools (list[BaseTool | Callable]): The tools to use in the API call.
    response_model (BaseModel | BaseType): The response model into which the response
        should be structured.
    output_parser (Callable[[OpenAICallResponse | ResponseModelT], Any]): A function for
        parsing the call response whose value will be returned in place of the original
        call response.
    json_mode (bool): Whether to use JSON Mode.
    client (None): xAI does not support a custom client.
    call_params (OpenAICallParams): The `OpenAICallParams` call parameters to use in the
        API call.

Returns:
    decorator (Callable): The decorator for turning a typed function into a xAI
        routed LLM API call.

<ParametersTable
  parameters={[
  {
    "name": "model",
    "type": "str",
    "module_context": "builtins",
    "is_builtin": "true",
    "description": "The model to use in the API call."
  },
  {
    "name": "stream",
    "type": "bool",
    "module_context": "builtins",
    "is_builtin": "true",
    "description": "Whether to stream the response from the API call."
  },
  {
    "name": "tools",
    "type": "list[BaseTool | Callable]",
    "module_context": "builtins",
    "is_builtin": "true",
    "description": "The tools to use in the API call."
  },
  {
    "name": "response_model",
    "type": "BaseModel | BaseType",
    "module_context": "mirascope.core.xai._call",
    "is_builtin": "false",
    "description": "The response model into which the response\nshould be structured."
  },
  {
    "name": "output_parser",
    "type": "Callable[[OpenAICallResponse | ResponseModelT], Any]",
    "module_context": "typing",
    "is_builtin": "true",
    "description": "A function for\nparsing the call response whose value will be returned in place of the original\ncall response."
  },
  {
    "name": "json_mode",
    "type": "bool",
    "module_context": "builtins",
    "is_builtin": "true",
    "description": "Whether to use JSON Mode."
  },
  {
    "name": "client",
    "type": "None",
    "module_context": "typing",
    "is_builtin": "true",
    "description": "xAI does not support a custom client."
  },
  {
    "name": "call_params",
    "type": "OpenAICallParams",
    "module_context": "mirascope.core.xai._call",
    "is_builtin": "false",
    "description": "The `OpenAICallParams` call parameters to use in the\nAPI call."
  }
]}
  contentSubpath="docs/mirascope"
  currentModule="mirascope.core.xai._call"
/>

<ReturnType
  type="Callable"
  isBuiltin={false}
  contentSubpath="docs/mirascope"
  currentModule="mirascope.core.xai._call"
  description="The decorator for turning a typed function into a xAI\nrouted LLM API call."
/>


